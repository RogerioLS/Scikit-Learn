{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding-up gradient-boosting\n",
    "\n",
    "Neste notebook, apresentamos uma versão modificada do aumento de gradiente que usa um número reduzido de divisões ao construir as diferentes árvores. Esse algoritmo é chamado de \"aumento de gradiente de histograma\" no scikit-learn.\n",
    "\n",
    "Mencionamos anteriormente que a floresta aleatória é um algoritmo eficiente, pois cada árvore do conjunto pode ser ajustada ao mesmo tempo de forma independente. Portanto, o algoritmo é escalonado de maneira eficiente com o número de núcleos e com o número de amostras.\n",
    "\n",
    "No aumento de gradiente, o algoritmo é um algoritmo sequencial. É necessário que as `N-1` árvores tenham sido ajustadas para poder se ajustar à árvore no estágio `N`. Portanto, o algoritmo é bastante caro em termos computacionais. A parte mais cara desse algoritmo é a busca pela melhor divisão na árvore, que é uma abordagem de força bruta: todas as divisões possíveis são avaliadas e a melhor é escolhida. Explicamos este processo no caderno \"árvore em profundidade\", ao qual você pode se referir.\n",
    "\n",
    "Para acelerar o algoritmo de aumento de gradiente, pode-se reduzir o número de divisões a serem avaliadas. Como consequência, o desempenho de generalização de tal árvore seria reduzido. No entanto, como estamos combinando várias árvores em um aumento de gradiente, podemos adicionar mais estimadores para superar esse problema.\n",
    "\n",
    "Faremos uma implementação ingênua de tal algoritmo usando blocos de construção do scikit-learn. Primeiro, carregaremos o conjunto de dados de habitação da Califórnia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\"> Se você quiser uma visão geral mais detalhada sobre este conjunto de dados, pode consultar o\n",
    "Apêndice - seção de descrição dos conjuntos de dados no final deste MOOC. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos uma rápida avaliação do aumento de gradiente original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=200)\n",
    "cv_results_gbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Decision Tree\n",
      "Mean absolute error via cross-validation: 46.389 +/- 2.910 k$\n",
      "Average fit time: 10.796 seconds\n",
      "Average score time: 0.022 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Boosting Decision Tree\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_gbdt['test_score'].mean():.3f} +/- \"\n",
    "      f\"{cv_results_gbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Average fit time: \"\n",
    "      f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Average score time: \"\n",
    "      f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembramos que uma forma de acelerar o aumento do gradiente é reduzir o número de divisões considerado dentro da construção da árvore. Uma maneira é agrupar os dados antes de colocá-los no aumento de gradiente. Um transformador chamado `KBinsDiscretizer` está fazendo essa transformação. Assim, podemos canalizar esse pré-processamento com o aumento de gradiente.\n",
    "\n",
    "Podemos primeiro demonstrar a transformação feita pelo `KBinsDiscretizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rogerio Lopes\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:200: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn('Bins whose width are too small (i.e., <= '\n",
      "C:\\Users\\Rogerio Lopes\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:200: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn('Bins whose width are too small (i.e., <= '\n",
      "C:\\Users\\Rogerio Lopes\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:200: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn('Bins whose width are too small (i.e., <= '\n",
      "C:\\Users\\Rogerio Lopes\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:200: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn('Bins whose width are too small (i.e., <= '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[249.,  39., 231., ...,  83., 162.,  30.],\n",
       "       [248.,  19., 203., ...,  28., 161.,  30.],\n",
       "       [242.,  49., 249., ..., 125., 160.,  29.],\n",
       "       ...,\n",
       "       [ 17.,  15., 126., ...,  49., 200.,  82.],\n",
       "       [ 23.,  16., 136., ...,  29., 200.,  77.],\n",
       "       [ 53.,  14., 130., ...,  93., 199.,  81.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(\n",
    "    n_bins=256, encode=\"ordinal\", strategy=\"quantile\")\n",
    "data_trans = discretizer.fit_transform(data)\n",
    "data_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\"> A célula de código acima irá gerar alguns avisos. Na verdade, para alguns dos recursos, solicitamos muitos bins em relação à dispersão de dados para esses recursos. Os menores escaninhos serão removidos.\n",
    "\n",
    "Vemos que o discretizador transforma os dados originais em um inteiro. Este inteiro representa o índice bin quando a distribuição por quantil é realizada. Podemos verificar o número de caixas por recurso. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256, 50, 256, 253, 256, 256, 207, 235]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(np.unique(col)) for col in data_trans.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após essa transformação, vemos que temos no máximo 256 valores exclusivos por recursos. Agora, usaremos esse transformador para discretizar os dados antes de treinar o regressor de aumento de gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "gradient_boosting = make_pipeline(\n",
    "    discretizer, GradientBoostingRegressor(n_estimators=200))\n",
    "cv_results_gbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Decision Tree with KBinsDiscretizer\n",
      "Mean absolute error via cross-validation: 45.806 +/- 2.194 k$\n",
      "Average fit time: 9.336 seconds\n",
      "Average score time: 0.036 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Boosting Decision Tree with KBinsDiscretizer\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_gbdt['test_score'].mean():.3f} +/- \"\n",
    "      f\"{cv_results_gbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Average fit time: \"\n",
    "      f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Average score time: \"\n",
    "      f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, vemos que o tempo de ajuste foi drasticamente reduzido, mas que o desempenho de generalização do modelo é idêntico. O Scikit-learn oferece classes específicas que são ainda mais otimizadas para grandes conjuntos de dados, chamadas HistGradientBoostingClassifier e HistGradientBoostingRegressor. Cada recurso nos dados do conjunto de dados é primeiro categorizado por histogramas de computação, que são usados posteriormente para avaliar as possíveis divisões. O número de divisões a avaliar é muito menor. Esse algoritmo se torna muito mais eficiente do que o aumento de gradiente quando o conjunto de dados tem mais de 10.000 amostras.\n",
    "\n",
    "A seguir, daremos um exemplo para um grande conjunto de dados e compararemos os tempos de computação com o experimento da seção anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "histogram_gradient_boosting = HistGradientBoostingRegressor(\n",
    "    max_iter=200, random_state=0)\n",
    "cv_results_hgbdt = cross_validate(\n",
    "    histogram_gradient_boosting, data, target,\n",
    "    scoring=\"neg_mean_absolute_error\", n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram Gradient Boosting Decision Tree\n",
      "Mean absolute error via cross-validation: 43.758 +/- 2.694 k$\n",
      "Average fit time: 3.142 seconds\n",
      "Average score time: 0.050 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Histogram Gradient Boosting Decision Tree\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_hgbdt['test_score'].mean():.3f} +/- \"\n",
    "      f\"{cv_results_hgbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Average fit time: \"\n",
    "      f\"{cv_results_hgbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Average score time: \"\n",
    "      f\"{cv_results_hgbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O aumento de gradiente do histograma é o melhor algoritmo em termos de pontuação. Ele também será dimensionado quando o número de amostras aumentar, enquanto o reforço de gradiente normal não."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
